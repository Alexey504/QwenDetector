import torch
import json
import os
from PIL import Image, ImageDraw, ImageFont
from qwen_vl_utils import process_vision_info
from .model_loader import get_model_and_processor
import tempfile
from typing import Tuple, List, Any


def _create_annotated_image(
        image: Image.Image,
        json_data: str,
        height: int,
        width: int
) -> Tuple[Image.Image, List[str]]:
    """
    Create an annotated version of the input image based on bounding box data provided in JSON format.

    The function parses bounding boxes and labels from a JSON string embedded in Markdown-style
    code fences (```json ... ```). It then draws rectangles and text labels over the image to
    visualize detected objects. Bounding boxes are rescaled to match the current image dimensions.

    Args:
        image (Image.Image): The original PIL image to annotate.
        json_data (str): A string containing JSON-formatted annotation data, possibly wrapped in
            triple backticks (```json ... ```).
        height (int): The original height of the image used when creating the annotations.
        width (int): The original width of the image used when creating the annotations.

    Returns:
        Tuple[Image.Image, List[str]]:
            - The annotated PIL image with bounding boxes and labels drawn on it.
            - A list of all label strings extracted from the annotations.

    Notes:
        - Each JSON object in `json_data` should contain a `bbox_2d`, `bbox`, or `box` key with
          [x1, y1, x2, y2] coordinates, and optionally a `label` key.
        - If parsing fails, the function returns the original image and an empty label list.
    """

    try:
        parsed_json_data = json_data.split("```json")[1].split("```")[0]
        bbox_data = json.loads(parsed_json_data)
    except Exception:
        return image, []

    annotated = image.convert("RGB")
    draw = ImageDraw.Draw(annotated)

    try:
        font = ImageFont.truetype("arial.ttf", 16)
    except:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 20)

    labels = []

    for obj in bbox_data:
        bbox = obj.get("bbox_2d") or obj.get("bbox") or obj.get("box") or []
        label = obj.get("label", "")
        labels.append(label)

        if len(bbox) == 4:
            x1, y1, x2, y2 = bbox
            scale_x = image.width / width
            scale_y = image.height / height
            x1, y1, x2, y2 = [int(x1 * scale_x), int(y1 * scale_y), int(x2 * scale_x), int(y2 * scale_y)]

            draw.rectangle([x1, y1, x2, y2], outline="magenta", width=3)

            try:
                text_bbox = draw.textbbox((0, 0), label, font=font)
                text_w = text_bbox[2] - text_bbox[0]
                text_h = text_bbox[3] - text_bbox[1]
            except AttributeError:
                text_w, text_h = font.getsize(label)

            text_bg = [x1, y1 - text_h - 4, x1 + text_w + 6, y1]
            draw.rectangle(text_bg, fill="magenta")
            draw.text((x1 + 3, y1 - text_h - 2), label, fill="white", font=font)

    return annotated, labels


def generate_visual_answer(image: Image.Image,
                           user_prompt: str
                           ) -> tuple[Image, str | Any] | None:
    """
    Generate a visual answer for a given image and text prompt using a multimodal model (e.g. Qwen-VL).

    The function sends the image and user prompt to a vision-language model,
    asking it first to produce object bounding boxes in JSON format (```json ... ```),
    followed by a short natural-language scene description.
    The JSON is then used to draw annotations on the image.

    Args:
        image (Image.Image): Input PIL image to analyze.
        user_prompt (str): Text prompt or question about the image.

    Returns:
        Tuple[Image.Image, str]:
            - Annotated image (`Image.Image`) with bounding boxes drawn according to model output.
            - Text description (`str`) of the image scene, generated by the model.

    Raises:
        ValueError: If the input image is empty or corrupted.
        RuntimeError: If model inference or processor steps fail.

    Notes:
        - The function uses `get_model_and_processor()` to load the multimodal model and its processor.
        - The model is expected to output both JSON annotations and a natural language caption.
        - Temporary files are automatically cleaned up.
        - GPU memory is cleared after inference to prevent accumulation.

    Example:
        annotated, desc = generate_visual_answer(img, "Что изображено на фото?")
        annotated.show()
        print(desc)
        "На фото показана улица с машинами и людьми."
    """

    model, processor = get_model_and_processor()

    with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp:
        temp_path = tmp.name
    try:
        image.save(temp_path)

        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": temp_path},
                {"type": "text", "text": (
                    f"{user_prompt}\n"
                    "Сначала верни JSON с координатами объектов в формате ```json```, "
                    "затем после JSON добавь короткое текстовое описание сцены."
                )},
            ],
        }]

        if image is None or image.size[0] == 0 or image.size[1] == 0:
            raise ValueError("Пустое или повреждённое изображение")

        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)

        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        ).to(model.device)

        with torch.inference_mode():
            generated_ids = model.generate(**inputs, max_new_tokens=1024)
            output_text = processor.batch_decode(
                generated_ids[:, inputs.input_ids.shape[1]:],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False,
            )[0]
    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)

    annotated, _ = _create_annotated_image(
        image=image,
        json_data=output_text,
        height=image.height,
        width=image.width,
    )

    # image description
    description = "Описание недоступно"
    if "```json" in output_text:
        try:
            description = output_text.split("```json")[1].split("```")[1].strip()
        except Exception:
            pass

    if torch.cuda.is_available():
        torch.cuda.synchronize()
        torch.cuda.empty_cache()

    return annotated, description
